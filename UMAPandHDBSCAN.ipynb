pwd

file_path ='/Users/briannaaustin/Desktop/lsngc(2)/EC_Brianna(2)'

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
import umap
import hdbscan
import matplotlib.pyplot as plt


def read_and_flatten(file_path):
    df = pd.read_csv(file_path, header=None)
    affinity_matrix = df.iloc[1:, 1:].values.astype(float)
    return affinity_matrix.flatten()

def process_directory(directory, label):
    X, y = [], []
    for filename in os.listdir(directory):
        if filename.endswith("_Aff.csv"):
            filepath = os.path.join(directory, filename)
            X.append(read_and_flatten(filepath))
            y.append(label)
    return X, y

def load_data():
    cue_hc_dir = "/Users/briannaaustin/Desktop/lsngc(2)/EC_Brianna(2)/CueData/HC_Cue"
    cue_patients_dir = "/Users/briannaaustin/Desktop/lsngc(2)/EC_Brianna(2)/CueData/Patients_Cue"
    mid_hc_dir = "/Users/briannaaustin/Desktop/lsngc(2)/EC_Brianna(2)/MIDData/HC_MID"
    mid_p_dir = "/Users/briannaaustin/Desktop/lsngc(2)/EC_Brianna(2)/MIDData/Patients_MID"

    X, y = [], []
    for directory, label in [(cue_hc_dir, 0), (cue_patients_dir, 1), (mid_hc_dir, 0), (mid_p_dir, 1)]:
        X_dir, y_dir = process_directory(directory, label)
        X.extend(X_dir)
        y.extend(y_dir)
    return np.array(X), np.array(y)


print("Loading data...")
X, y = load_data()
print(f"Data shape: {X.shape}, Labels shape: {y.shape}")


scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)


X_scaled = np.nan_to_num(X_scaled)
#UMAP parameters 
umap_n_neighbors = 10
umap_min_dist = 0.1
umap_n_components = 10

#Applying UMAP
print("Applying UMAP...")
reducer = umap.UMAP(
    n_neighbors=umap_n_neighbors,
    min_dist=umap_min_dist,
    n_components=umap_n_components,
    random_state=42
)
X_reduced = reducer.fit_transform(X_scaled)
print(f"UMAP reduced data shape: {X_reduced.shape}")


#Clustering, specifically chosen HDBSCAN.
hdbscan_min_cluster_size = 5
hdbscan_min_samples = 10

# Apply HDBSCAN
print("Applying HDBSCAN...")
hdbscan_model = hdbscan.HDBSCAN(
    min_samples=hdbscan_min_samples,
    min_cluster_size=hdbscan_min_cluster_size,
    metric='euclidean'
)
clusters = hdbscan_model.fit_predict(X_reduced)

#Cluster eval metrics.
# SS, cluster purity
print("Evaluating clustering...")
non_noise_mask = clusters != -1
silhouette_avg = silhouette_score(X_reduced[non_noise_mask], clusters[non_noise_mask])
print(f"Silhouette Score: {silhouette_avg:.4f}")

# Cluster Purity
def calculate_purity(clusters, labels):
    unique_clusters = np.unique(clusters)
    purity = []
    for cluster_id in unique_clusters:
        if cluster_id == -1:  # Skip noise points
            continue
        cluster_mask = clusters == cluster_id
        cluster_labels = labels[cluster_mask]
        dominant_group = max(np.sum(cluster_labels == 0), np.sum(cluster_labels == 1))
        purity.append(dominant_group / len(cluster_labels))
    return np.mean(purity)

cluster_purity = calculate_purity(clusters, y)
print(f"Cluster Purity: {cluster_purity:.4f}")


#Cluster profiling, also telling me about the noise points.
def cluster_composition_analysis(clusters, labels):
    unique_clusters = np.unique(clusters)
    results = []

    for cluster_id in unique_clusters:
        if cluster_id == -1:  # Skip noise points
            continue
        cluster_mask = clusters == cluster_id
        hc_count = np.sum((labels == 0) & cluster_mask)
        patient_count = np.sum((labels == 1) & cluster_mask)
        total_count = hc_count + patient_count

        results.append({
            'Cluster': cluster_id,
            'HC Count': hc_count,
            'Patient Count': patient_count,
            'Total': total_count,
            'HC %': (hc_count / total_count) * 100 if total_count > 0 else 0,
            'Patient %': (patient_count / total_count) * 100 if total_count > 0 else 0,
        })

    return pd.DataFrame(results)

cluster_composition = cluster_composition_analysis(clusters, y)
print("\nCluster Composition:")
print(cluster_composition)


noise_mask = clusters == -1  #Noise points are labelled as -1, so this way it is only them being identified
num_noise_points = np.sum(noise_mask) 
total_points = len(clusters) 


noise_proportion = (num_noise_points / total_points) * 100

# Patient and HC composition in noise
num_hc_in_noise = np.sum((y == 0) & noise_mask)
num_patients_in_noise = np.sum((y == 1) & noise_mask)


hc_proportion_in_noise = (num_hc_in_noise / num_noise_points) * 100 if num_noise_points > 0 else 0
patient_proportion_in_noise = (num_patients_in_noise / num_noise_points) * 100 if num_noise_points > 0 else 0


print("\nNoise Point Analysis:")
print(f"Total Noise Points: {num_noise_points} ({noise_proportion:.2f}%)")
print(f"HC in Noise: {num_hc_in_noise} ({hc_proportion_in_noise:.2f}%)")
print(f"Patients in Noise: {num_patients_in_noise} ({patient_proportion_in_noise:.2f}%)")


# Visualize clusters including noise

plt.figure(figsize=(10, 8))
plt.scatter(
    X_reduced[clusters != -1, 0],  # UMAP dimension 1 for clustered points
    X_reduced[clusters != -1, 1],  # UMAP dimension 2 for clustered points
    c=clusters[clusters != -1],  # Use cluster labels for color
    cmap='viridis',  # Colormap for clusters
    label='Clusters',
    alpha=0.7,  # Transparency
    s=30  # Marker size
)
plt.scatter(
    X_reduced[clusters == -1, 0],  # UMAP dimension 1 for noise points
    X_reduced[clusters == -1, 1],  # UMAP dimension 2 for noise points
    c='red',  # Color for noise points
    label='Noise',
    alpha=0.5,  # Transparency for noise points
    s=30  # Marker size
)
plt.title("UMAP Embedding with Clusters and Noise Points", fontsize=16)
plt.xlabel("UMAP Dimension 1", fontsize=12)
plt.ylabel("UMAP Dimension 2", fontsize=12)
plt.legend(loc="best", fontsize=10)
plt.grid(True, alpha=0.3)
plt.show()
